# -*- coding: utf-8 -*-
"""facialKeyPointsDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qjVPBNhitFpiBgFONJQ7hck6_rWSZh5a
"""

# Commented out IPython magic to ensure Python compatibility.
# get the dataset from kaggle
# https://www.kaggle.com/c/facial-keypoints-detection
# copy data into train & test folder

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline

from keras.layers.advanced_activations import LeakyReLU
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Convolution2D,
                                     BatchNormalization,
                                     Flatten)
from tensorflow.keras.layers import (Dense,
                                     Dropout,
                                     MaxPool2D)

train = pd.read_csv("train/training.csv")
test = pd.read_csv("test/test.csv")
lookup = pd.read_csv("IdLookupTable.csv")
sample = pd.read_csv("SampleSubmission.csv")

train.isnull().sum()

train.fillna(method='ffill', inplace=True)

imag = []
for i in range(0, 7049):
    img = train["Image"][i].split(' ')
    img = ['0' if x == '' else x for x in img]
    imag.append(img)

image_list = np.array(imag, dtype="float")
X_train = image_list.reshape(-1, 96, 96, 1)

plt.imshow(X_train[5].reshape(96, 96), cmap='gray')
plt.show()

training = train.drop('Image', axis=1)

y_train = []
for i in range(0, 7049):
    y = training.iloc[i, :]

    y_train.append(y)
y_train = np.array(y_train, dtype='float')

y_train = pd.DataFrame(y_train)
y_train.head()

model = Sequential()

# layer set 1
model.add(Convolution2D(32,
                        (3, 3),
                        padding='same',
                        use_bias=False,
                        input_shape=(96, 96, 1)
                        ))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(Convolution2D(32, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
# result 1 image is converted into 48 X 48 X 32 = 73K

# layer set 2
model.add(Convolution2D(64, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())

model.add(Convolution2D(64, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
# outputs 24 X 24 X 64 = 36K

# layer set 3
model.add(Convolution2D(96, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())

model.add(Convolution2D(96, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
# outputs 12 X 12 X 96 = 14K

# layer set 4
model.add(Convolution2D(128, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())

model.add(Convolution2D(128, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
# outputs 6 X 6 X 128 = 4K

# layer set 5
model.add(Convolution2D(256, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())

model.add(Convolution2D(256, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
# outputs 3 X 3 X 256 = 2K

# different set
model.add(Convolution2D(512, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())

model.add(Convolution2D(512, (3, 3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
# outputs 3 X 3 X 512 = 4K

# Normal layer
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(30))

model.summary()

model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=['mae']
              )

model.fit(X_train, y_train, epochs=50, batch_size=256, validation_split=0.2)

model.save('facialKeyPointsDetection.h5')

# preparing test data
timag = []
for i in range(0, 1783):
    timg = test['Image'][i].split(' ')
    timg = ['0' if x == '' else x for x in timg]

    timag.append(timg)

timage_list = np.array(timag, dtype='float')
X_test = timage_list.reshape(-1, 96, 96, 1)

plt.imshow(X_test[0].reshape(96, 96), cmap='gray')
plt.show()

pred = model.predict(X_test)
pred[0]
